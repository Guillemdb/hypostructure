---
title: "RESOLVE-AutoAdmit - Complexity Theory Translation"
---

# RESOLVE-AutoAdmit: Automatic Admissibility Verification

## Original Theorem (Hypostructure Context)

The RESOLVE-AutoAdmit theorem establishes that surgery admissibility can be **automatically verified** from thin objects without user-provided admissibility code. The framework computes:
1. Singular locus identification via dissipation rate
2. Profile extraction via scaling limits
3. Capacity computation via variational methods
4. Codimension bounds via dimension-capacity relationships

**Core insight:** Admissibility checking is decidable for good classes, requiring no user intervention.

**Original Theorem Reference:** {prf:ref}`mt-resolve-auto-admit`

---

## Complexity Theory Statement

**Theorem (Automatic Verification / Decidable Type Checking).**
Let $\mathcal{T} = (\Gamma, \tau, \vdash)$ be a type system with context $\Gamma$, types $\tau$, and typing judgment $\vdash$. Suppose $\mathcal{T}$ satisfies the **Decidability Conditions**:
1. **Finite canonical forms:** The set of normal forms at each type is decidable
2. **Bounded normalization:** Type-directed reduction terminates in bounded steps
3. **Local constraints:** Typing rules are syntax-directed

Then:

1. **Type checking is decidable:** Given $\Gamma, e, \tau$, the judgment $\Gamma \vdash e : \tau$ is decidable.

2. **Type inference is semi-decidable:** Given $\Gamma, e$, if a type exists, it can be found algorithmically.

3. **Proof checking is automatic:** For proof terms $\pi$ and propositions $P$, verification $\pi : P$ requires no user-provided checking code.

**Informal:** For well-behaved type systems (good classes), the framework automatically verifies whether terms have their claimed types without external oracles.

---

## Terminology Translation Table

| Hypostructure Concept | Complexity Theory Equivalent |
|-----------------------|------------------------------|
| Thin objects $(\mathcal{X}^{\text{thin}}, \mathfrak{D}^{\text{thin}})$ | Type context $\Gamma$ and term syntax |
| Singular locus $\Sigma$ | Constraint set / subtyping obligations |
| Canonical library $\mathcal{L}_T$ | Normal forms / canonical types |
| Profile extraction | Type inference / normalization |
| Capacity $\text{Cap}(\Sigma)$ | Constraint complexity measure |
| Codimension bound $\geq 2$ | Constraint set of measure zero (negligible) |
| Admissibility certificate $K_{\text{adm}}$ | Type derivation / proof certificate |
| Good type $T$ | Decidable type fragment |
| Automation Guarantee | Decidability of type checking |
| Surgery admissibility | Term well-typedness |
| Metric measure space $(\mathcal{X}, d, \mu)$ | Term structure with complexity measure |
| Dissipation rate $R$ | Reduction potential / normalization fuel |
| Library membership | Normal form recognition |
| Variational capacity computation | SAT/SMT constraint solving |
| Hausdorff dimension bound | Complexity class containment |

---

## Proof Sketch

### Setup: Decidable Type Systems

**Definition (Decidable Type System).**
A type system $\mathcal{T} = (\Gamma, \tau, \vdash)$ is decidable if there exists an algorithm $\textsc{TypeCheck}$ such that:
$$\textsc{TypeCheck}(\Gamma, e, \tau) = \begin{cases} \text{ACCEPT} & \text{if } \Gamma \vdash e : \tau \\ \text{REJECT} & \text{otherwise} \end{cases}$$

and $\textsc{TypeCheck}$ terminates on all inputs.

**Definition (Constraint Set).**
For a typing problem $(\Gamma, e, \tau)$, the constraint set is:
$$\Sigma(\Gamma, e, \tau) = \{C_1, \ldots, C_k\}$$
where each $C_i$ is a subtyping or equality constraint generated by syntax-directed rules.

**Definition (Constraint Capacity).**
The capacity of constraint set $\Sigma$ is:
$$\text{Cap}(\Sigma) = \text{cost of solving } \Sigma \text{ via SMT/unification}$$

This measures the computational effort to verify all constraints simultaneously.

---

### Step 1: Constraint Generation (Singular Locus Identification)

**Claim:** Type checking reduces to constraint generation followed by constraint solving.

**Algorithm (Constraint Generation):**
```
ConstraintGen(Gamma, e, tau):
  Input: Context Gamma, term e, claimed type tau
  Output: Constraint set Sigma

  match e with:
  | Var(x) ->
      return {Gamma(x) = tau}

  | App(e1, e2) ->
      fresh alpha
      Sigma1 := ConstraintGen(Gamma, e1, alpha -> tau)
      Sigma2 := ConstraintGen(Gamma, e2, alpha)
      return Sigma1 ∪ Sigma2

  | Lam(x, e') ->
      fresh alpha, beta
      Sigma' := ConstraintGen(Gamma ∪ {x:alpha}, e', beta)
      return Sigma' ∪ {alpha -> beta = tau}

  | ... (other constructs)
```

**Correspondence to Hypostructure:**
- Constraint generation $\leftrightarrow$ Singular locus identification
- Each constraint $C_i$ $\leftrightarrow$ Local singularity data
- Constraint set $\Sigma$ $\leftrightarrow$ Global singular locus

**Termination:** Constraint generation terminates in $O(|e|)$ steps where $|e|$ is the term size.

---

### Step 2: Constraint Solving (Capacity Computation)

**Claim:** Constraint satisfiability determines type checking outcome.

**Algorithm (Constraint Solving via Unification):**
```
SolveConstraints(Sigma):
  Input: Constraint set Sigma = {C_1, ..., C_k}
  Output: SATISFIABLE with substitution theta, or UNSATISFIABLE

  theta := identity substitution
  worklist := Sigma

  while worklist non-empty:
    pick C from worklist
    match C with:
    | tau = tau' where tau, tau' are ground ->
        if tau ≠ tau': return UNSATISFIABLE
    | alpha = tau where alpha is variable ->
        if alpha occurs in tau: return UNSATISFIABLE (occurs check)
        theta := [alpha -> tau] ∘ theta
        worklist := apply [alpha -> tau] to worklist
    | (tau1 -> tau2) = (tau1' -> tau2') ->
        add {tau1 = tau1', tau2 = tau2'} to worklist
    | ... (other decomposition rules)

  return SATISFIABLE(theta)
```

**Correspondence to Hypostructure:**
- Unification $\leftrightarrow$ Variational capacity computation
- Substitution $\theta$ $\leftrightarrow$ Optimal test function $\phi^*$
- Occurs check failure $\leftrightarrow$ Infinite capacity
- Successful unification $\leftrightarrow$ Finite capacity bound

**Complexity:**
- Hindley-Milner type inference: $O(n \cdot \alpha(n))$ where $\alpha$ is inverse Ackermann
- General unification: decidable but can be PSPACE-complete
- First-order unification: $O(n)$ time (Robinson, Martelli-Montanari)

---

### Step 3: Normal Form Recognition (Canonicity Check)

**Claim:** Checking membership in canonical forms is decidable for good type fragments.

**Definition (Canonical Form Library).**
For type system $\mathcal{T}$, the canonical form library is:
$$\mathcal{L}_\mathcal{T} = \{v : v \text{ is a value/normal form of } \mathcal{T}\}$$

**Algorithm (Canonicity Check):**
```
CanonicalCheck(e, tau):
  Input: Term e, type tau
  Output: CANONICAL, CANONICAL_SIM, or NON_CANONICAL

  // Normalize the term
  v := normalize(e)

  // Check if v is in canonical form
  if IsValue(v):
    if SyntacticCanonical(v, tau):
      return CANONICAL
    else if exists e' such that e ≈ e' and SyntacticCanonical(e', tau):
      return CANONICAL_SIM  // Up to equivalence
    else:
      return NON_CANONICAL
  else:
    return NON_CANONICAL  // Normalization did not produce value
```

**Correspondence to Hypostructure:**
- `normalize(e)` $\leftrightarrow$ Profile extraction via scaling limit
- `IsValue(v)` $\leftrightarrow$ Library membership test
- `CANONICAL` $\leftrightarrow$ $V \in \mathcal{L}_T$
- `CANONICAL_SIM` $\leftrightarrow$ $K_{\text{adm}}^\sim$ (admissible up to equivalence)

**Decidability:** For strongly normalizing systems (e.g., simply-typed lambda calculus, System F with stratification), normalization terminates and canonical form checking is decidable.

---

### Step 4: Dimension/Complexity Bounds (Codimension Check)

**Claim:** The "codimension" of constraint sets corresponds to complexity class containment.

**Definition (Constraint Codimension).**
For constraint set $\Sigma$ on type universe of dimension $n$ (number of type variables):
$$\text{codim}(\Sigma) = n - \dim(\text{solution space of } \Sigma)$$

**Interpretation:**
- $\text{codim}(\Sigma) \geq 2$: Constraints are "thin" / well-constrained
- $\text{codim}(\Sigma) < 2$: Constraints are "fat" / under-constrained or over-constrained

**Complexity Class Correspondence:**
| Codimension Condition | Complexity Property |
|-----------------------|---------------------|
| $\text{codim}(\Sigma) \geq 2$ | Polynomial-time solvable |
| $\text{codim}(\Sigma) = 1$ | Linear-time solvable |
| $\text{codim}(\Sigma) = 0$ | Trivially satisfiable |
| $\text{codim}(\Sigma) < 0$ | Overconstrained (likely UNSAT) |

**Algorithm (Codimension Verification):**
```
CodimensionCheck(Sigma, n):
  Input: Constraint set Sigma, dimension n
  Output: TRUE if codim >= 2, FALSE otherwise

  // Count degrees of freedom
  num_vars := count_type_variables(Sigma)
  num_constraints := |Sigma|

  // Compute effective dimension
  rank := compute_constraint_rank(Sigma)  // Via Gaussian elimination on constraint matrix
  dim_solution := num_vars - rank

  codim := n - dim_solution

  return codim >= 2
```

---

### Step 5: SMT Solving (Automatic Verification)

**Claim:** Modern SMT solvers provide automatic verification for rich constraint languages.

**SMT (Satisfiability Modulo Theories) Framework:**

**Definition (SMT Problem).**
Given a formula $\varphi$ in a theory $T$ (e.g., linear arithmetic, arrays, bitvectors), determine:
$$T \models \varphi \quad \text{(is $\varphi$ valid in $T$?)}$$

or equivalently:
$$T \models \neg\varphi \text{ UNSAT} \quad \text{(is $\neg\varphi$ unsatisfiable?)}$$

**Algorithm (SMT-Based Type Checking):**
```
SMT_TypeCheck(Gamma, e, tau):
  Input: Context Gamma, term e, type tau
  Output: VERIFIED, REFUTED, or UNKNOWN

  // Generate verification conditions
  Sigma := ConstraintGen(Gamma, e, tau)

  // Encode constraints in SMT logic
  phi := encode_to_SMT(Sigma)

  // Call SMT solver
  result := SMT_Solver(phi)

  match result with:
  | SAT(model) -> return VERIFIED(model_to_derivation(model))
  | UNSAT(proof) -> return REFUTED(proof)
  | UNKNOWN -> return UNKNOWN
```

**Correspondence to Hypostructure:**
- SMT formula $\varphi$ $\leftrightarrow$ Singular locus description
- SAT model $\leftrightarrow$ Admissibility certificate $K_{\text{adm}}$
- UNSAT proof $\leftrightarrow$ Inadmissibility witness $K_{\text{inadm}}$
- SMT decidability $\leftrightarrow$ Automation Guarantee

**Key SMT Theories for Type Checking:**
| Theory | Application |
|--------|-------------|
| Equality with Uninterpreted Functions (EUF) | Type equality |
| Linear Integer Arithmetic (LIA) | Size constraints |
| Bitvectors (BV) | Fixed-width types |
| Arrays | Dependent types with indexing |
| Algebraic Datatypes | Inductive types |

---

### Step 6: Certificate Construction

**Theorem (Decidability of Admissibility).**
For type systems satisfying the Decidability Conditions, the typing judgment is decidable and produces explicit certificates.

**Certificate Schema:**

**Case 1 (Well-Typed):**
$$K_{\text{type}}^+ = (\Gamma, e, \tau, \mathcal{D})$$
where $\mathcal{D}$ is the type derivation tree.

**Case 2 (Well-Typed up to Coercion):**
$$K_{\text{type}}^\sim = (\Gamma, e, \tau', c, K_{\text{type}}^+[\tau'])$$
where $c: \tau' \to \tau$ is a coercion and $K_{\text{type}}^+[\tau']$ certifies $\Gamma \vdash e : \tau'$.

**Case 3 (Ill-Typed):**
$$K_{\text{type}}^- = (\text{failure\_location}, \text{constraint\_witness})$$
where `failure_location` identifies the subterm causing the error and `constraint_witness` is the unsatisfiable constraint.

**Algorithm (Automatic Admissibility Trichotomy):**
```
AutoAdmit(Gamma, e, tau):
  Input: Typing problem (Gamma, e, tau)
  Output: Certificate K ∈ {K_type^+, K_type^~, K_type^-}

  // Step 1: Generate constraints
  Sigma := ConstraintGen(Gamma, e, tau)

  // Step 2: Check canonicity (normal form)
  e_norm := normalize(e)
  (canon_status, equiv_data) := CanonicalCheck(e_norm, tau)

  if canon_status = NON_CANONICAL:
    return K_type^-("Non-normalizing term", e)

  // Step 3: Solve constraints (capacity computation)
  (sat_status, theta_or_proof) := SolveConstraints(Sigma)

  if sat_status = UNSATISFIABLE:
    return K_type^-("Type mismatch", theta_or_proof)

  // Step 4: Verify codimension (constraint complexity)
  if not CodimensionCheck(Sigma, dim(tau)):
    return K_type^-("Constraint set too complex", Sigma)

  // Step 5: Construct certificate
  theta := theta_or_proof
  D := build_derivation(Gamma, e, tau, theta)

  if canon_status = CANONICAL:
    return K_type^+(Gamma, e, tau, D)
  else:  // CANONICAL_SIM
    return K_type^~(Gamma, e, tau, equiv_data, D)
```

---

## Connections to Dependent Types

### Dependent Type Theory

In dependent type theory (e.g., Martin-Lof Type Theory, Coq, Agda), types can depend on terms:
$$\Pi(x : A). B(x)$$

**Challenge:** Type checking becomes undecidable in general because it requires evaluating arbitrary terms.

**Solution (Normalization-by-Evaluation):** Restrict to strongly normalizing fragments where:
1. All terms normalize to canonical forms
2. Normalization is decidable
3. Type equality reduces to syntactic equality of normal forms

**Correspondence to RESOLVE-AutoAdmit:**
| Hypostructure | Dependent Types |
|---------------|-----------------|
| Good type $T$ | Strongly normalizing fragment |
| Canonical library $\mathcal{L}_T$ | Normal forms |
| Profile extraction | Normalization-by-evaluation |
| Capacity bound | Normalization complexity bound |
| Automation Guarantee | Decidability of conversion checking |

### Bidirectional Type Checking

**Algorithm (Bidirectional):**
```
// Checking mode: verify e has type tau
check(Gamma, e, tau):
  match e with:
  | Lam(x, body) ->
      match tau with:
      | Pi(x, A, B) -> check(Gamma, x:A, body, B)
      | _ -> error

// Inference mode: synthesize type of e
infer(Gamma, e):
  match e with:
  | Var(x) -> return Gamma(x)
  | App(e1, e2) ->
      Pi(x, A, B) := infer(Gamma, e1)
      check(Gamma, e2, A)
      return B[e2/x]
```

**Correspondence:** Bidirectional checking mirrors the capacity computation:
- Checking mode $\leftrightarrow$ Verifying $\text{Cap}(\Sigma) \leq \varepsilon_{\text{adm}}$
- Inference mode $\leftrightarrow$ Computing optimal capacity bound

---

## Connections to SMT Solving

### DPLL(T) Algorithm

Modern SMT solvers use the DPLL(T) architecture:
```
DPLL(T)(phi):
  // Propositional backbone: SAT solver
  while True:
    model := DPLL_SAT(phi)
    if model = UNSAT:
      return UNSAT

    // Theory solver: check consistency
    result := Theory_Check(model)
    if result = OK:
      return SAT(model)
    else:
      conflict_clause := result
      phi := phi ∧ conflict_clause
```

**Correspondence to RESOLVE-AutoAdmit:**
| SMT Component | Hypostructure Component |
|---------------|------------------------|
| Boolean skeleton | Topological structure of $\Sigma$ |
| Theory solver | Capacity computation |
| Conflict clause | Codimension violation witness |
| Model | Admissibility certificate |

### Decidable Theories for Type Systems

| Theory | Decidability | Complexity | Type System Application |
|--------|--------------|------------|------------------------|
| Propositional logic | Decidable | NP-complete | Simple types |
| EUF (Equality + Uninterpreted Functions) | Decidable | $O(n \log n)$ | ML type inference |
| Linear arithmetic (LIA/LRA) | Decidable | Polynomial/Exponential | Refinement types |
| Presburger arithmetic | Decidable | 3EXPTIME | Size types |
| First-order arithmetic | Undecidable | - | Full dependent types |

---

## Connections to Proof Checking

### Curry-Howard Correspondence

The Curry-Howard correspondence identifies:
- Types $\leftrightarrow$ Propositions
- Terms $\leftrightarrow$ Proofs
- Type checking $\leftrightarrow$ Proof verification

**RESOLVE-AutoAdmit in Proof-Theoretic Terms:**

| Hypostructure | Proof Theory |
|---------------|--------------|
| Surgery admissibility | Proof validity |
| Thin objects | Proof term structure |
| Canonical library | Normal proofs |
| Capacity bound | Proof complexity |
| Automatic verification | Proof checking decidability |

### Proof Assistants

Modern proof assistants (Coq, Lean, Agda) implement RESOLVE-AutoAdmit:

**Coq's Kernel:**
```ocaml
(* Type checking is decidable because: *)
(* 1. Reduction is strongly normalizing *)
(* 2. Conversion checking compares normal forms *)
(* 3. All operations are syntax-directed *)

let rec type_check env term expected_type =
  let inferred = type_infer env term in
  let inferred_nf = normalize inferred in
  let expected_nf = normalize expected_type in
  if syntactic_equal inferred_nf expected_nf then
    Success
  else
    Error (type_mismatch inferred expected_type)
```

**Automation Guarantee:** The proof assistant verifies proofs without requiring user-provided verification code. Users write proof terms; the kernel checks them automatically.

---

## Worked Example: Simply-Typed Lambda Calculus

**Problem:** Check if $\lambda x. \lambda y. x$ has type $\alpha \to \beta \to \alpha$.

**Step 1 (Constraint Generation):**
```
ConstraintGen(∅, λx.λy.x, α→β→α)
  = ConstraintGen({x:α}, λy.x, β→α)
  = ConstraintGen({x:α, y:β}, x, α)
  = {α = α}  // Lookup x in context
```

**Step 2 (Constraint Solving):**
```
Sigma = {α = α}
SolveConstraints({α = α}) = SATISFIABLE(identity)
```

**Step 3 (Canonicity Check):**
```
e_norm = λx.λy.x  // Already in normal form
CanonicalCheck(λx.λy.x, α→β→α) = CANONICAL
```

**Step 4 (Codimension Check):**
```
num_vars = 2 (α, β)
num_constraints = 1
rank = 0 (tautological constraint)
codim = 2 - 2 = 0  // Fully determined, passes
```

**Step 5 (Certificate):**
```
K_type^+ = (∅, λx.λy.x, α→β→α, D)
where D =
  ─────────────────────────── (Var)
  x:α, y:β ⊢ x : α
  ─────────────────────────── (Abs)
  x:α ⊢ λy.x : β→α
  ─────────────────────────── (Abs)
  ∅ ⊢ λx.λy.x : α→β→α
```

---

## Worked Example: SMT-Based Refinement Type Checking

**Problem:** Verify that `abs : (x:Int) -> {v:Int | v >= 0}` is well-typed.

**Step 1 (Constraint Generation):**
```
// From the definition: abs(x) = if x >= 0 then x else -x
// Generate refinement constraints:
Sigma = {
  x >= 0 => v = x => v >= 0,    // then-branch
  x < 0 => v = -x => v >= 0     // else-branch
}
```

**Step 2 (SMT Encoding):**
```smt2
(declare-const x Int)
(declare-const v Int)

; Verification condition
(assert (not (and
  (=> (and (>= x 0) (= v x)) (>= v 0))
  (=> (and (< x 0) (= v (- x))) (>= v 0))
)))

(check-sat)  ; Expected: UNSAT (condition is valid)
```

**Step 3 (Solver Result):**
```
Result: UNSAT
=> Verification conditions are valid
=> Function is well-typed
```

**Step 4 (Certificate):**
```
K_type^+ = (
  Gamma = ∅,
  e = λx. if x >= 0 then x else -x,
  tau = (x:Int) -> {v:Int | v >= 0},
  witness = SMT_UNSAT_proof
)
```

---

## Theoretical Implications

### Decidability Landscape

| Type System | Type Checking | Type Inference | Fragment |
|-------------|---------------|----------------|----------|
| Simply-typed lambda calculus | Decidable | Decidable | Good |
| System F (polymorphic) | Decidable | Undecidable | Good for checking |
| System F$_\omega$ | Decidable | Undecidable | Good for checking |
| Dependent types (CoC) | Decidable | Undecidable | Good for checking |
| Full dependent types | Undecidable | Undecidable | Not good |

### Automation Guarantee Hierarchy

The RESOLVE-AutoAdmit theorem stratifies type systems by automation level:

1. **Full Automation:** Simple types, ML with let-polymorphism
   - Type inference is complete
   - No annotations required

2. **Partial Automation:** System F, dependent types
   - Type checking is decidable
   - Annotations required for polymorphic bindings

3. **SMT-Assisted:** Refinement types, liquid types
   - Decidable for SMT-decidable theories
   - May timeout on complex constraints

4. **Interactive:** Full dependent types, homotopy type theory
   - User provides proof terms
   - Kernel verifies automatically

---

## Summary

The RESOLVE-AutoAdmit theorem, translated to complexity theory, establishes:

**For type systems satisfying the Decidability Conditions (good classes), type checking is automatically computable without user-provided verification code.**

This principle manifests as:
1. **Decidable type checking:** Constraint generation + solving is algorithmic
2. **Automatic proof verification:** The Curry-Howard correspondence ensures proofs are checkable
3. **SMT integration:** Modern solvers provide automatic constraint solving
4. **Certificate construction:** Well-typed terms produce derivations; ill-typed terms produce witnesses

The translation reveals deep connections between:
- **Hypostructure admissibility** and **type system decidability**
- **Capacity bounds** and **constraint complexity**
- **Canonical libraries** and **normal forms**
- **Automation Guarantee** and **decidability of conversion**

This unifies the singularity-resolution framework with foundational results in type theory and automated reasoning.
