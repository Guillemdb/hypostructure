---
title: "THM-NonCircularity - AI/RL/ML Translation"
---

# THM-NonCircularity: Non-Circular Training Definitions

## Overview

The non-circularity theorem establishes that training procedures, loss functions, and evaluation metrics must be defined without circular dependencies. Circular definitions lead to degenerate solutions, training collapse, or undefined behavior.

**Original Theorem Reference:** {prf:ref}`thm-non-circularity`

---

## AI/RL/ML Statement

**Theorem (Non-Circularity, ML Form).**
A training system $(\mathcal{L}, f_\theta, \mathcal{D}, \text{Eval})$ is well-defined if:

1. **Loss Independence:** $\mathcal{L}$ does not depend on its own output
2. **Evaluation Independence:** Eval metric independent of training process
3. **Target Independence:** Labels/rewards not generated by model being trained

**Circularity Detection:** A system has circular dependency if:
$$\exists \text{ cycle in } \mathcal{L} \to f_\theta \to \mathcal{D} \to \mathcal{L}$$

**Consequence of Circularity:** Training may:
- Converge to trivial solution
- Oscillate indefinitely
- Collapse to degenerate mode

---

## Terminology Translation Table

| Hypostructure Concept | AI/RL/ML Analog | Formal Correspondence |
|-----------------------|-----------------|------------------------|
| Circularity | Training cycle | Feedback loop in definitions |
| Non-circularity | Well-posed training | Acyclic dependency graph |
| Foundational axioms | Ground truth | Fixed targets/labels |
| Derived concepts | Predictions | Model outputs |
| Definitional hierarchy | Training pipeline | Order of computations |
| Self-reference | Self-distillation | Model trains on own outputs |

---

## Types of Circularity in ML

### Circularity Classification

| Type | Example | Problem |
|------|---------|---------|
| Label circularity | Self-training on predictions | Confirmation bias |
| Loss circularity | Loss depends on loss | Undefined gradient |
| Reward circularity | Reward from agent | Gaming |
| Evaluation circularity | Test on training data | Overfitting |
| Architecture circularity | Output feeds input | Infinite regress |

### Dependency Graph

```
Well-formed:
Data → Labels → Loss → Gradients → Parameters

Circular (problematic):
Data → Predictions → Labels → Loss → Predictions (cycle!)
```

---

## Proof Sketch

### Step 1: Formal Dependency Graph

**Claim:** Training has a dependency structure.

**Components:**
- $\mathcal{D}$: Training data (ground truth)
- $f_\theta$: Model (parameterized)
- $\mathcal{L}$: Loss function
- $\nabla$: Gradient computation
- $\text{Update}$: Parameter update

**Acyclic Requirement:**
$$\mathcal{D} \to f_\theta \to \mathcal{L} \to \nabla \to \text{Update} \to f_\theta$$

The cycle through Update is acceptable (training loop), but $\mathcal{D}$ must not depend on $f_\theta$.

### Step 2: Label Circularity

**Claim:** Self-generated labels cause collapse.

**Problem Setting:**
$$y_t = f_{\theta_{t-1}}(x), \quad \theta_t = \text{Update}(\theta_{t-1}, \mathcal{L}(f_{\theta_t}(x), y_t))$$

**Degenerate Fixed Point:**
$$f_\theta(x) = f_\theta(x) \quad \forall x$$

Any constant function satisfies this.

**Remedy:** Fixed or external labels, or regularization.

**Reference:** Lee, D.-H. (2013). Pseudo-label. *ICML Workshop*.

### Step 3: Reward Circularity in RL

**Claim:** Self-defined rewards lead to reward hacking.

**Problem:**
$$r_t = f_\theta(s_t, a_t) \quad \text{(agent predicts own reward)}$$

**Collapse:** Agent learns $r_t = \max$ always.

**Safe RL Design:**
- External reward signal
- Fixed reward function
- Human feedback (RLHF)

**Reference:** Amodei, D., et al. (2016). Concrete problems in AI safety. *arXiv*.

### Step 4: GAN Mode Collapse

**Claim:** GAN training has controlled circularity.

**GAN Objective:**
$$\min_G \max_D V(D, G) = \mathbb{E}[\log D(x)] + \mathbb{E}[\log(1 - D(G(z)))]$$

**Circularity:** G depends on D, D depends on G.

**Resolution:** Alternating optimization breaks cycle:
1. Fix G, update D
2. Fix D, update G

**Failure Mode:** Simultaneous update can oscillate.

**Reference:** Goodfellow, I., et al. (2014). Generative adversarial nets. *NeurIPS*.

### Step 5: Self-Distillation Non-Circularity

**Claim:** Self-distillation is non-circular when properly staged.

**Born-Again Networks:**
$$\mathcal{L} = D_{\text{KL}}(f_{\theta_{\text{student}}}(x) \| f_{\theta_{\text{teacher}}}(x))$$

**Non-Circularity:** Teacher is frozen (past version).

**Circularity Avoided:** Student $\neq$ current teacher.

**Reference:** Furlanello, T., et al. (2018). Born-again neural networks. *ICML*.

### Step 6: Evaluation Circularity

**Claim:** Evaluation must be independent of training.

**Circular Evaluation:**
$$\text{Eval} = \mathcal{L}_{\text{train}}(\theta) \quad \text{(same data)}$$

**Problem:** Overfitting appears as good performance.

**Non-Circular:**
$$\text{Eval} = \mathcal{L}_{\text{test}}(\theta), \quad \mathcal{D}_{\text{test}} \cap \mathcal{D}_{\text{train}} = \emptyset$$

**Cross-Validation:** Rotates roles to maintain separation.

### Step 7: Bootstrapping in RL

**Claim:** Temporal difference has controlled circularity.

**TD Target:**
$$y_t = r_t + \gamma V_\theta(s_{t+1})$$

**Circularity:** $V_\theta$ appears in both LHS and RHS.

**Resolution:** Target network $\theta^-$ (delayed copy):
$$y_t = r_t + \gamma V_{\theta^-}(s_{t+1})$$

**Reference:** Mnih, V., et al. (2015). Human-level control with deep RL. *Nature*.

### Step 8: Contrastive Learning Non-Circularity

**Claim:** Contrastive methods avoid label circularity.

**SimCLR:**
$$\mathcal{L} = -\log\frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_k \exp(\text{sim}(z_i, z_k)/\tau)}$$

**Non-Circularity:** Positives from augmentation (fixed), not model.

**Self-Supervised but Non-Circular:** Labels from data augmentation, not predictions.

**Reference:** Chen, T., et al. (2020). SimCLR. *ICML*.

### Step 9: Curriculum and Bootstrapping

**Claim:** Curriculum learning is non-circular.

**Curriculum:**
$$\mathcal{D}_t = \{(x, y) : \text{difficulty}(x) \leq t\}$$

**Non-Circularity:** Difficulty defined externally (not by model).

**Bootstrap Circularity (to avoid):**
$$\text{difficulty}(x) = \mathcal{L}(f_\theta(x), y)$$

Using current model to define difficulty creates cycle.

### Step 10: Compilation Theorem

**Theorem (Non-Circularity):**

A training system is well-defined iff its dependency graph is acyclic (except for the training loop itself):

1. **Labels:** From external source, not model
2. **Loss:** Depends on model output and external target
3. **Evaluation:** On held-out data
4. **Rewards (RL):** From environment, not agent

**Non-Circularity Certificate:**
$$K_{\text{NC}} = (\text{DAG of dependencies}, \text{no } \mathcal{L} \to f \to \mathcal{D} \to \mathcal{L} \text{ cycles})$$

**Applications:**
- Training pipeline verification
- Self-supervised learning design
- RL reward design
- Evaluation protocol validation

---

## Key AI/ML Techniques Used

1. **Dependency Graph:**
   $$\mathcal{D} \to f_\theta \to \mathcal{L} \to \nabla \to \theta$$

2. **Target Network:**
   $$y = r + \gamma V_{\theta^-}(s')$$

3. **Train-Test Split:**
   $$\mathcal{D}_{\text{train}} \cap \mathcal{D}_{\text{test}} = \emptyset$$

4. **Alternating Optimization:**
   $$\theta_G^{t+1} = \text{Update}(\theta_G^t | \theta_D^t)$$

---

## Literature References

- Goodfellow, I., et al. (2014). Generative adversarial nets. *NeurIPS*.
- Mnih, V., et al. (2015). Human-level control. *Nature*.
- Lee, D.-H. (2013). Pseudo-label. *ICML Workshop*.
- Chen, T., et al. (2020). SimCLR. *ICML*.
- Amodei, D., et al. (2016). Concrete problems in AI safety. *arXiv*.

