---
title: "KRNL-Shadowing - AI/RL/ML Translation"
---

# KRNL-Shadowing: Sim-to-Real Transfer Guarantees

## Original Statement (Hypostructure)

**[KRNL-Shadowing] Shadowing Metatheorem.** Let $\mathcal{H}$ be a Hypostructure with:
1. A Stiffness certificate: $K_{\mathrm{LS}_\sigma}^+$ with spectral gap $\lambda > 0$
2. A numerical pseudo-orbit: $\{y_n\}$ with $d(f(y_n), y_{n+1}) < \varepsilon$ for all $n$
3. Hyperbolicity: the tangent map $Df$ has exponential dichotomy

**Statement:** For every $\varepsilon$-pseudo-orbit (numerical simulation), there exists a true orbit $\{x_n\}$ that $\delta(\varepsilon)$-shadows it: $d(x_n, y_n) < \delta(\varepsilon)$ for all $n$. The shadowing distance satisfies $\delta(\varepsilon) = O(\varepsilon/\lambda)$.

**Certificate Logic:**
$$K_{\mathrm{LS}_\sigma}^+ \wedge K_{\text{pseudo}}^{\varepsilon} \Rightarrow K_{\text{true}}^{\delta(\varepsilon)}$$

## AI/RL/ML Statement

**Theorem (Sim-to-Real Transfer Guarantee).** Let $\mathcal{M}_{\text{sim}}$ be a simulated environment and $\mathcal{M}_{\text{real}}$ be the real-world environment. Assume:

1. **Value Stability (Spectral Gap):** The value function $V^\pi(s)$ has a spectral gap $\lambda > 0$ in its temporal difference operator, ensuring exponential convergence of value estimates
2. **Simulated Rollout (Pseudo-Orbit):** A trajectory $\{s_n^{\text{sim}}, a_n^{\text{sim}}\}$ generated by policy $\pi$ in $\mathcal{M}_{\text{sim}}$ with dynamics mismatch $\|P_{\text{sim}}(\cdot|s,a) - P_{\text{real}}(\cdot|s,a)\|_1 < \varepsilon$
3. **Contractive Dynamics (Hyperbolicity):** The system exhibits exponential stability with contraction rate $\mu > 0$ in the state-value mapping

**Statement:** For every simulated rollout with sim-to-real gap $\varepsilon$, there exists a real-world trajectory $\{s_n^{\text{real}}\}$ that tracks it within distance $\delta(\varepsilon) = O(\varepsilon/\lambda)$. The policy $\pi$ achieves similar cumulative reward in reality:
$$\left| V^{\pi}(s_0; \mathcal{M}_{\text{real}}) - V^{\pi}(s_0; \mathcal{M}_{\text{sim}}) \right| \leq \frac{C \varepsilon}{\lambda (1 - \gamma)}$$

**RL Certificate:**
$$[\text{Spectral-Gap } \lambda > 0] \wedge [\text{Sim-Rollout with error } \varepsilon] \Rightarrow [\text{Real-Trajectory within } O(\varepsilon/\lambda)]$$

## Terminology Translation Table

| Hypostructure | AI/RL/ML |
|---------------|----------|
| Height function $\Phi$ | Value function $V(s)$ or $Q(s,a)$ |
| Dissipation $D$ | Policy $\pi(a\|s)$ (action distribution) |
| Pseudo-orbit $\{y_n\}$ | Simulated rollout / trajectory from simulator |
| True orbit $\{x_n\}$ | Real-world execution / deployment trajectory |
| Shadowing distance $\delta$ | Sim-to-real gap / transfer error |
| Pseudo-orbit error $\varepsilon$ | Dynamics mismatch / model error |
| Spectral gap $\lambda$ | TD operator contraction rate / value stability margin |
| Exponential dichotomy | Stable/unstable decomposition of value gradient dynamics |
| Stiffness certificate $K_{\mathrm{LS}_\sigma}^+$ | Verified value function stability |
| Time-one map $f$ | One-step transition under policy $\pi$ |
| Phase space $X$ | State space $\mathcal{S}$ |
| Shadowing property | Trajectory tracking / rollout fidelity |
| Banach fixed-point argument | Bellman iteration convergence |
| Green's function representation | Multi-step value propagation |
| Contraction mapping | TD operator contraction |

## Proof Sketch

### Setup: Sim-to-Real as Shadowing

**Definition (Simulated Rollout as Pseudo-Orbit).** A simulated rollout $\tau_{\text{sim}} = \{s_0^{\text{sim}}, a_0, s_1^{\text{sim}}, a_1, \ldots\}$ is an $\varepsilon$-pseudo-orbit for the real dynamics if:
$$d(T_{\text{real}}(s_n^{\text{sim}}, a_n), s_{n+1}^{\text{sim}}) < \varepsilon$$
where $T_{\text{real}}$ is the real transition function and $d$ is a state-space metric.

**Definition (Dynamics Mismatch).** The sim-to-real dynamics gap is:
$$\varepsilon_{\text{dynamics}} = \sup_{s,a} \|P_{\text{sim}}(\cdot|s,a) - P_{\text{real}}(\cdot|s,a)\|_{\text{TV}}$$

**Definition (Spectral Gap of TD Operator).** For policy $\pi$ with discount $\gamma$, the TD operator is:
$$(\mathcal{T}^\pi V)(s) = R(s, \pi(s)) + \gamma \sum_{s'} P(s'|s, \pi(s)) V(s')$$

The spectral gap is $\lambda = 1 - \gamma \rho(\mathcal{P}^\pi)$ where $\rho(\mathcal{P}^\pi)$ is the spectral radius of the transition kernel. For ergodic chains, $\lambda > 0$ ensures exponential convergence.

### Step 1: Trajectory Deviation Dynamics

**State Deviation Sequence.** Define the deviation between real and simulated trajectories:
$$\xi_n := s_n^{\text{real}} - s_n^{\text{sim}}$$

**Linearized Deviation Dynamics.** Expanding the real dynamics around the simulated trajectory:
$$s_{n+1}^{\text{real}} = T_{\text{real}}(s_n^{\text{real}}, a_n) = T_{\text{real}}(s_n^{\text{sim}} + \xi_n, a_n)$$
$$\approx T_{\text{real}}(s_n^{\text{sim}}, a_n) + \nabla_s T_{\text{real}}|_{s_n^{\text{sim}}} \cdot \xi_n + O(\|\xi_n\|^2)$$

**Deviation Equation.** The deviation evolves as:
$$\xi_{n+1} = J_n \xi_n + e_n + R_n(\xi_n)$$
where:
- $J_n = \nabla_s T_{\text{real}}|_{s_n^{\text{sim}}}$ is the Jacobian of real dynamics
- $e_n = T_{\text{real}}(s_n^{\text{sim}}, a_n) - s_{n+1}^{\text{sim}}$ is the one-step dynamics mismatch with $\|e_n\| < \varepsilon$
- $R_n(\xi_n) = O(\|\xi_n\|^2)$ is the nonlinear remainder

### Step 2: Spectral Gap Implies Bounded Propagation

**Stable-Unstable Decomposition.** The spectral gap hypothesis implies the Jacobian $J_n$ has eigenvalues bounded away from the unit circle:
$$\sigma(J_n) \cap \{z : |z| \in (e^{-\lambda}, e^{\lambda})\} = \emptyset$$

This creates a splitting of the tangent space into stable and unstable subspaces:
- **Stable subspace $E^s$:** Perturbations decay as $\|J^k|_{E^s}\| \leq C e^{-\mu k}$
- **Unstable subspace $E^u$:** Perturbations grow but can be controlled backward as $\|(J^k)^{-1}|_{E^u}\| \leq C e^{-\mu k}$

**Value Function Interpretation.** In RL terms:
- **Stable directions:** States from which the policy reliably reaches high-value regions
- **Unstable directions:** States requiring precise control to avoid value degradation

### Step 3: Fixed-Point Construction of Shadowing Trajectory

**Sequence Space Formulation.** Work in the Banach space:
$$\ell^\infty = \{\{\xi_n\}_{n \geq 0} : \sup_n \|\xi_n\| < \infty\}$$

**Shadowing Operator.** Define $\mathcal{T}: \ell^\infty \to \ell^\infty$ via Green's function:
$$(\mathcal{T}\xi)_n = \sum_{k=0}^{n-1} G_n^{n-k}(e_k + R_k(\xi_k)) - \sum_{k=n}^{\infty} \tilde{G}_n^{k-n}(e_k + R_k(\xi_k))$$
where:
- $G_n^k$ propagates stable components forward with decay $\|G_n^k\| \leq C e^{-\mu k}$
- $\tilde{G}_n^k$ propagates unstable components backward with decay $\|\tilde{G}_n^k\| \leq C e^{-\mu k}$

**Contraction Estimate.** For the ball $B_\rho = \{\xi : \|\xi\|_\infty \leq \rho\}$ with $\rho = 2C\varepsilon/\lambda$:

1. **Self-mapping:** $\mathcal{T}: B_\rho \to B_\rho$ when $\varepsilon$ is small enough
2. **Contraction:** $\|\mathcal{T}\xi - \mathcal{T}\eta\|_\infty \leq \theta \|\xi - \eta\|_\infty$ with $\theta < 1$

**Smallness Condition:** Contraction holds when:
$$\varepsilon < \varepsilon_0 = \frac{\lambda^2}{4C^2 K}$$
where $K$ is the Lipschitz constant of the dynamics.

**Existence of Shadowing Trajectory.** By Banach fixed-point theorem, there exists unique $\xi^* \in B_\rho$ with:
$$\|\xi_n^*\| \leq \frac{2C\varepsilon}{\lambda}$$

The real trajectory $s_n^{\text{real}} = s_n^{\text{sim}} + \xi_n^*$ shadows the simulated rollout with distance $\delta(\varepsilon) = O(\varepsilon/\lambda)$.

### Step 4: Value Function Transfer Bound

**Value Difference Decomposition.** The sim-to-real value gap is:
$$\Delta V = V^\pi(s_0; \mathcal{M}_{\text{real}}) - V^\pi(s_0; \mathcal{M}_{\text{sim}})$$

**Trajectory-Based Bound.** Using the shadowing trajectory:
$$\Delta V = \sum_{t=0}^\infty \gamma^t \left[ R(s_t^{\text{real}}, a_t) - R(s_t^{\text{sim}}, a_t) \right]$$

**Reward Lipschitz Bound.** If $R$ is $L_R$-Lipschitz in states:
$$|R(s_t^{\text{real}}, a_t) - R(s_t^{\text{sim}}, a_t)| \leq L_R \|s_t^{\text{real}} - s_t^{\text{sim}}\| \leq L_R \delta(\varepsilon)$$

**Total Value Gap:**
$$|\Delta V| \leq \sum_{t=0}^\infty \gamma^t L_R \delta(\varepsilon) = \frac{L_R \delta(\varepsilon)}{1 - \gamma} = \frac{2 C L_R \varepsilon}{\lambda (1 - \gamma)}$$

### Step 5: Certificate Construction

**Sim-to-Real Transfer Certificate.** The proof yields:

**Input Certificate** $K_{\text{sim}}^{\varepsilon}$:
- Simulated rollout: $\tau_{\text{sim}} = \{s_n^{\text{sim}}, a_n\}$
- Dynamics mismatch bound: $\varepsilon$ with $\|P_{\text{sim}} - P_{\text{real}}\|_{\text{TV}} < \varepsilon$

**Stability Certificate** $K_{\text{spectral}}^{\lambda}$:
- Spectral gap: $\lambda > 0$
- Contraction constants: $C, \mu$ for stable/unstable decomposition

**Output Certificate** $K_{\text{transfer}}^{\delta}$:
- Real trajectory: $\{s_n^{\text{real}}\}$ tracking simulated rollout
- Tracking distance: $\delta(\varepsilon) = 2C\varepsilon/\lambda$
- Value transfer bound: $|\Delta V| \leq \frac{2 C L_R \varepsilon}{\lambda (1 - \gamma)}$

**Certificate Logic:**
$$K_{\text{spectral}}^{\lambda} \wedge K_{\text{sim}}^{\varepsilon} \Rightarrow K_{\text{transfer}}^{\delta(\varepsilon)}$$

## Connections to AI/RL/ML

### Sim-to-Real Transfer

**Classical Sim-to-Real Problem.** Train policy $\pi$ in simulator $\mathcal{M}_{\text{sim}}$, deploy in reality $\mathcal{M}_{\text{real}}$. The transfer gap arises from:
- Physics engine inaccuracies
- Sensor/actuator modeling errors
- Unmodeled disturbances

**Shadowing Interpretation.** The simulated rollout is a pseudo-orbit of the real dynamics. Shadowing guarantees that if the dynamics mismatch $\varepsilon$ is small relative to the spectral gap $\lambda$, there exists a real trajectory close to the simulated one.

**Key Insight:** The spectral gap $\lambda$ acts as a "robustness margin." Larger spectral gap $\Rightarrow$ more tolerance for sim-real mismatch:
$$\delta(\varepsilon) = O(\varepsilon / \lambda)$$

**When Shadowing Fails.** If the real dynamics have near-zero spectral gap (near-neutral stability), small simulator errors cause large trajectory deviations. This explains why sim-to-real transfer is hard for:
- Underactuated systems near instability
- High-dimensional systems with slow modes
- Contact-rich manipulation (discontinuous dynamics)

### Domain Adaptation

**Source and Target Domains.** Let source domain be the simulator and target domain be reality:
- Source: $\mathcal{D}_S = (\mathcal{S}, \mathcal{A}, P_{\text{sim}}, R)$
- Target: $\mathcal{D}_T = (\mathcal{S}, \mathcal{A}, P_{\text{real}}, R)$

**Shadowing as Domain Adaptation.** Trajectories in the source domain (pseudo-orbits) are corrected to trajectories in the target domain (true orbits). The correction distance is bounded by:
$$d_{\text{trajectory}}(\tau_S, \tau_T) \leq \frac{C \cdot d_{\text{dynamics}}(P_S, P_T)}{\lambda}$$

**Advantage over Distribution Matching.** Traditional domain adaptation matches feature distributions. Shadowing provides trajectory-level guarantees:
- Distribution matching: $\mathbb{E}_{\tau_S}[\phi(\tau)] \approx \mathbb{E}_{\tau_T}[\phi(\tau)]$ for features $\phi$
- Shadowing: $d(\tau_S, \tau_T) \leq \delta$ pointwise along trajectories

### Transfer Learning

**Pre-training and Fine-tuning.** Train value function $V_S$ in source domain, transfer to target:
$$V_T(s) \approx V_S(s) + \text{correction}$$

**Shadowing-Based Transfer Bound.** The value function transfer error is:
$$\|V_T - V_S\|_\infty \leq \frac{L_R \cdot \delta(\varepsilon)}{1 - \gamma} = \frac{C L_R \varepsilon}{\lambda (1 - \gamma)}$$

**Fine-tuning Interpretation.** If the pre-trained value function $V_S$ is good enough (within $\delta$ of $V_T$), local fine-tuning converges to $V_T$. The spectral gap ensures the Bellman operator contracts, preventing divergence during fine-tuning.

**Catastrophic Forgetting Connection.** Large spectral gap $\Rightarrow$ stable fixed point $\Rightarrow$ fine-tuning converges without destroying pre-trained knowledge.

### Model-Based RL

**Learned Dynamics Model.** In model-based RL, we learn $\hat{P}(s'|s,a) \approx P(s'|s,a)$ and plan using the model.

**Model Error as Pseudo-Orbit Error.** Trajectories from the learned model are pseudo-orbits of the true dynamics:
$$\varepsilon_{\text{model}} = \sup_{s,a} \|\hat{P}(\cdot|s,a) - P(\cdot|s,a)\|_{\text{TV}}$$

**Shadowing for Model-Based Planning.** If planning produces trajectory $\tau_{\text{model}}$ in the learned model, there exists a true trajectory $\tau_{\text{real}}$ within distance:
$$d(\tau_{\text{model}}, \tau_{\text{real}}) \leq \frac{C \varepsilon_{\text{model}}}{\lambda}$$

**Model Quality Requirement.** For successful model-based RL, we need:
$$\varepsilon_{\text{model}} < \frac{\lambda^2}{4C^2 K} = \varepsilon_0$$

This quantifies the model accuracy needed for reliable planning.

## Implementation Notes

### Estimating the Spectral Gap

**Method 1: Jacobian Analysis.** Compute Jacobian of learned dynamics model and estimate spectral gap:
```python
def estimate_spectral_gap(dynamics_model, trajectory, policy):
    """Estimate spectral gap along a trajectory."""
    min_gap = float('inf')
    for s, a in trajectory:
        # Compute Jacobian of dynamics at (s, a)
        J = jacobian(lambda x: dynamics_model(x, a), s)
        eigenvalues = np.linalg.eigvals(J)

        # Spectral gap = distance from eigenvalues to unit circle
        distances = np.abs(np.abs(eigenvalues) - 1.0)
        gap = distances.min()
        min_gap = min(min_gap, gap)

    return min_gap
```

**Method 2: TD Operator Analysis.** Estimate spectral gap of TD operator empirically:
```python
def estimate_td_spectral_gap(value_fn, policy, env, n_samples=1000):
    """Estimate spectral gap via TD iteration convergence rate."""
    V_old = value_fn.copy()
    convergence_rates = []

    for _ in range(n_samples):
        V_new = bellman_backup(V_old, policy, env)
        rate = np.linalg.norm(V_new - V_old) / np.linalg.norm(V_old - V_star)
        convergence_rates.append(rate)
        V_old = V_new

    # Spectral gap from geometric convergence rate
    mean_rate = np.mean(convergence_rates)
    spectral_gap = -np.log(mean_rate)
    return spectral_gap
```

### Sim-to-Real Transfer Algorithm

**Algorithm 1: Shadowing-Certified Sim-to-Real Transfer**
```
Input: Policy pi, simulator M_sim, real env M_real, error tolerance delta_max
Output: Deployed policy pi_deployed, transfer certificate C

1. Collect simulated rollout:
   tau_sim = rollout(pi, M_sim, horizon=H)

2. Estimate dynamics mismatch:
   epsilon = estimate_dynamics_gap(M_sim, M_real, tau_sim)

3. Estimate spectral gap along trajectory:
   lambda = estimate_spectral_gap(M_real, tau_sim, pi)

4. Compute shadowing distance bound:
   delta = 2 * C * epsilon / lambda

5. Check transfer feasibility:
   if delta > delta_max:
       return FAIL("Sim-real gap too large for spectral gap")

6. Deploy with monitoring:
   tau_real = rollout(pi, M_real, horizon=H)
   actual_deviation = max_t d(tau_sim[t], tau_real[t])

7. Construct certificate:
   C = (epsilon, lambda, delta, actual_deviation)

8. Return pi_deployed, C
```

**Algorithm 2: Domain Randomization with Shadowing**
```
Input: Nominal simulator params theta_0, perturbation range Delta
Output: Robust policy pi, shadowing radius eta

1. Sample perturbed simulators:
   theta_i ~ Uniform(theta_0 - Delta, theta_0 + Delta)
   M_i = Simulator(theta_i)

2. Train policy on mixture:
   pi = train_PPO(mixture of M_i)

3. Evaluate spectral gaps across simulators:
   lambda_i = estimate_spectral_gap(M_i, pi) for each i
   lambda_min = min(lambda_i)

4. Estimate max dynamics variation:
   epsilon_max = max_{i,j} dynamics_gap(M_i, M_j)

5. Compute shadowing radius:
   eta = 2 * C * epsilon_max / lambda_min

6. Report robustness:
   print(f"Policy robust to dynamics perturbations up to {eta}")

7. Return pi, eta
```

### Practical Considerations

**When Spectral Gap is Small.** If $\lambda \to 0$, the shadowing distance $\delta \to \infty$. Mitigations:
1. **Add damping:** Increase discount factor or add friction to stabilize dynamics
2. **Restrict state space:** Avoid regions with near-neutral stability
3. **Use robust control:** Design policies that increase effective spectral gap

**Handling Model Uncertainty.** If dynamics are uncertain:
```python
def robust_shadowing_bound(epsilon_dynamics, lambda_lower, lambda_upper, C):
    """Compute shadowing bound under parameter uncertainty."""
    # Use worst-case (smallest) spectral gap
    delta_worst = 2 * C * epsilon_dynamics / lambda_lower
    delta_best = 2 * C * epsilon_dynamics / lambda_upper
    return delta_worst, delta_best
```

**Online Adaptation.** If sim-real gap is large, adapt online:
```python
def adaptive_transfer(pi_sim, M_real, max_steps):
    """Adapt policy online using shadowing-guided correction."""
    pi = pi_sim.copy()

    for step in range(max_steps):
        # Collect real trajectory
        tau_real = rollout(pi, M_real, horizon=10)

        # Estimate current sim-real deviation
        delta = estimate_trajectory_deviation(tau_sim, tau_real)

        # If deviation exceeds threshold, update policy
        if delta > threshold:
            # Use deviation as reward signal for adaptation
            pi = update_policy(pi, tau_real, delta)

        # Check if converged
        if delta < convergence_threshold:
            break

    return pi
```

### Failure Modes and Mitigations

**Issue 1: Near-Zero Spectral Gap.** System is marginally stable.
- *Symptom:* Large $\delta$ even for small $\varepsilon$
- *Mitigation:* Add regularization to policy to increase stability margin
- *Mitigation:* Constrain policy to avoid marginally stable regions

**Issue 2: Large Dynamics Mismatch.** Simulator is inaccurate.
- *Symptom:* $\varepsilon > \varepsilon_0$ violates smallness condition
- *Mitigation:* Improve simulator fidelity
- *Mitigation:* Use system identification to reduce mismatch
- *Mitigation:* Domain randomization to cover uncertainty

**Issue 3: Non-Hyperbolic Dynamics.** No clean stable/unstable decomposition.
- *Symptom:* Eigenvalues on or near unit circle
- *Mitigation:* Use time-varying or local spectral gap estimates
- *Mitigation:* Apply shadowing only in hyperbolic regions

**Issue 4: Discontinuous Dynamics.** Contact, friction, mode switches.
- *Symptom:* Jacobian undefined or discontinuous
- *Mitigation:* Smooth approximations of contact dynamics
- *Mitigation:* Piecewise shadowing with mode matching

## Literature References

### Sim-to-Real Transfer
- Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., Abbeel, P. (2017). Domain randomization for transferring deep neural networks from simulation to the real world. *IROS*.
- Peng, X. B., Andrychowicz, M., Zaremba, W., Abbeel, P. (2018). Sim-to-real transfer of robotic control with dynamics randomization. *ICRA*.
- Sadeghi, F., Levine, S. (2017). CAD2RL: Real single-image flight without a single real image. *RSS*.
- Muratore, F., Ramos, F., Turk, G., Yu, W., Gienger, M., Peters, J. (2022). Robot learning from randomized simulations: A review. *Frontiers in Robotics and AI*.

### Domain Adaptation in RL
- Gamrian, S., Goldberg, Y. (2019). Transfer learning for related reinforcement learning tasks via image-to-image translation. *ICML*.
- Eysenbach, B., Ibarz, J., Gupta, A., Levine, S. (2020). Off-dynamics reinforcement learning: Training for transfer with domain classifiers. *arXiv*.
- Desai, S., Durugkar, I., Karber, H., Stone, P. (2020). An imitation from observation approach to transfer learning with dynamics mismatch. *NeurIPS*.

### Model-Based RL and Model Error
- Janner, M., Fu, J., Zhang, M., Levine, S. (2019). When to trust your model: Model-based policy optimization. *NeurIPS*.
- Chua, K., Calandra, R., McAllister, R., Levine, S. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. *NeurIPS*.
- Yu, T., Thomas, G., Yu, L., Erber, S., Gildea, S., Pieter, A., Levine, S. (2020). MOPO: Model-based offline policy optimization. *NeurIPS*.

### Shadowing in Dynamical Systems (Original Sources)
- Anosov, D. V. (1967). Geodesic flows on closed Riemannian manifolds of negative curvature. *Trudy Mat. Inst. Steklov*, 90, 3-210.
- Bowen, R. (1975). $\omega$-limit sets for Axiom A diffeomorphisms. *Journal of Differential Equations*, 18(2), 333-339.
- Palmer, K. J. (1988). Exponential dichotomies, the shadowing lemma and transversal homoclinic points. *Dynamics Reported*, 1, 265-306.
- Pilyugin, S. Yu. (1999). *Shadowing in Dynamical Systems*. Springer.

### Verified Simulation and Computer-Assisted Proofs
- Zgliczy≈Ñski, P., Mischaikow, K. (2001). Rigorous numerics for partial differential equations: The Kuramoto-Sivashinsky equation. *Foundations of Computational Mathematics*, 1(3), 255-288.
- Tucker, W. (1999). The Lorenz attractor exists. *Comptes Rendus de l'Academie des Sciences*, 328(12), 1197-1202.
- Van den Berg, J. B., Lessard, J. P. (2015). Rigorous numerics in dynamics. *Notices of the AMS*, 62(9), 1057-1061.

### Stability and Robustness in RL
- Berkenkamp, F., Turchetta, M., Schoellig, A., Krause, A. (2017). Safe model-based reinforcement learning with stability guarantees. *NeurIPS*.
- Chow, Y., Ghavamzadeh, M., Janson, L., Pavone, M. (2017). Risk-constrained reinforcement learning with percentile risk criteria. *JMLR*.
- Tessler, C., Mankowitz, D. J., Mannor, S. (2019). Reward constrained policy optimization. *ICLR*.
